Lift Logic
-------------------------------------------------------------------------------
General Theory:

For a general understanding of the lift algorithm, consider the figure on page
2 of the accompanying Lifts.odg document (open in Libreoffice) entitled "Simple
Lift Example."

You will also benefit from reviewing the article at:
  http://gotchoices.org/mychips/acdc.html

This example considers 4 trading entities.  In this example, there are 2 people
and 2 companies.  The arrows indicate the normal flow of credits, or credit
promises (think of payments).  So presumably, products and services are flowing 
in the opposite direction.

For example, person B works for company A and shops at company C.  Person D
works for company C and shops at copmany A.

Entities do not always have to alternate between companies and people in this 
way.  But generally, you are selling something (product, services, or labor) to 
the entities upstream from you and you are buying something (product, services
or labor) from the entites downstream.  This can really include any combination
of people and companies.

As products and services are delivered, payment is exchanged in the form of
private credit, or an IOU.  As this is an accounting function, it has a dual
entry nature (debit and credit).  A credit (negative, liability) is entered for 
the entity making payment.  And a debit (positive, asset) is entered for the
entity receiving payment.

If the signs of the transaction didn't seem to make sense, please visit:
  http://gotchoices.org/book/account.html

Keep in mind, these debits and credits themselves do not actually flow 
all the way around the circle.  For example, if A gives an IOU to B, that 
particular IOU will never flow along to C.  Since this is a private credit 
model, it means just that.  Your credit is private.  It is just between you 
and the entity you do business with directly--not some other entity you don't 
know or trust.

However, the goal is to make these credits fungible, or in our case, 
"effectively fungible."  That means we can't literally substitute one entity's 
credits for another, but through the lift algorithm, we will accomplish 
something roughly equivalent.

After a sufficient volume of trading, each entity will have accumulated a 
certain amount of asset value in IOU's from the entity upstream from him.  As
you can imagine, this can only go on so long.  For example, if B gets too many
IOU's from A, eventually he won't want any more.  He has reached his 
maximum "trust level."

What B really needs is some IOUs from C where B buys his stuff.  The lift 
function will allow B to give up some of his A credits in exchange for some C 
credits, just as he needs to complete his purchases.

But in order to accomplish this, we need to find a complete circuit.  This
is where entity D comes in so handy.  If we can, all at once, send a fixed 
number of credits around the circuit in the upstream direction (against the 
direction of the arrows), everyone's excessive buildup of IOU's can be 
relieved a little bit without costing anyone anything.

Everyone around the circuit will benefit from this transaction because they
will all give up credits they don't need in exchange for credits they do need.
However, we will need to do this in a way that no one gets hurt--particularly
if someone in the circle tries to cheat.

One way to accomplish this is to use a transactional database model.  The idea
is to que up all four of our credit exchanges belonging to a single lift and
make them part of an atomic database transaction.  This means the database will 
attempt all 4 transfers, but if any one of them fails, the other 3 will roll 
back too. So the lift will either work in its entirety, or it will not work at 
all.  The main goal is to make sure we don't perform only part of the lift, 
leaving someone out so they end up giving credits away, but not getting the 
ones they expect in return.

Now the bad news.  In order to do this, we would have to have all our entities 
in a single database.  That means our system would be too centralized and 
unscalable.  Centralization is prone to manipulation and corruption.  
Scalability is the critical feature we need to surpass what alternative systems 
like blockchain have been unable to achieve.

It would be nice to have an algorithm that will achieve an atomic result even 
when all 4 of our users have their data in different databases on different 
computers distributed around the Internet.  

And now, the bad news:

That may be impossible.  See:
  https://en.wikipedia.org/wiki/Two_Generals%27_Problem

It turns out this is a pretty old problem.  It is hard enough to get a group
of updates to operate as a single transaction inside a single database.  But
when you are trying to do this over unreliable links, or with other peers you
may not know or be able to fully trust, this just may not be achievable.

So to get over this otherwise insurmountable hurdle, we are going to make a
simple compromise:  Instead of making sure no one gets hurt during a lift, we
will instead, be satisfied if we can make sure no honest people get hurt.

To do so, we will set up our transaction in several basic phases:

In the first phase, we will discover circuits in the network which might be 
capable of supporting a lift.

In the next phase, we will initiate a proposed lift around that circuit.  Each
peer who is willing to participate on the suggested terms will give a 
conditional commitment.  This commitment is structured as a digital contract
which, when properly signed, becomes a binding CHIP credit.  In other words,
it is money to the recipient and debt to the issuer.  It just needs a final
signature.

In the final phase, we want to lock down each conditional credit and make it
real, official and binding.  This is where the 2 generals dilemma would bite us
if we were insisting on a fully transactional commit.  It is possible for
someone to conditionally commit in the prior phase, but then fail to cooperate
in the final commitment phase.

However, because everyone already posesses conditional credits, lacking only
the proper digital signature, all we really need is a way to get that signature
out to everyone and they will have their money.  Any deadbeat who drops off the
network in between these two phases will simply miss out.  He has already given
his conditional approval in the previous phase, so his immediate peer can prove
he is part of the lift and owes the value.  It is only the deadbeat himself who
is lacking the final credits he needs to be made whole.

This is not to say, everyone is happy about the deadbeat.  His upstream partner
may have a potential collection problem (at least if his balance runs into 
negative numbers).  But that is a risk he signed up for when initiating a tally
with the deadbeat.  Hopefully he has collateral or some other way to collect if
credit has been extended in the first place.

The deadbeat's partner on the downstream side has the opposite effect.  He got
the credits he wanted in the lift, but doesn't have to give anything up until
the deadbeat gets back online and completes the transaction.

The deadbeat has the further problem that he has burned his bridges to the
network of trust.  The two entities who know and trust him, now know he is up
to no good.  No further traffic can be performed by this entity--at least
through the peers he has just disappointed.

So the challenge in implementing the distributed lift function is this:  During
the conditional commit phase, the peers need to negotiate a chain of signatures
around the circuit which the lift originator can fully validate, proving that
each pair of peers along the route has agreed to the transaction, pending the
signature of the initiator himself.  Then, the initiator can form this final
signature and send it back around the loop.  If he sends it in both directions
(up and down stream), everyone involved in the process, except a single 
deadbeat can get the final signature they need.  

If there are two deadbeats in a single lift, it is possible for them to
deprive everyone in between them of the final signing key.  This could
conceivably be orchestrated between two corrupt nodes on the network, working
together and using a modified version of the server.

However, they can only do damage if the lift will run them into credit 
territory (i.e. a negative balance) with the party they are attemping
to burn.  That credit relationship won't last for long, and is a risk the
issuer of credit knowingly signed up for.

-------------------------------------------------------------------------------
Linear Lifts:

The lift algorithm described above is performed in a complete circle.  It's
function is to relieve credit surpluses and deficits around a complete circuit
of trading entities.

So the assumption is, a bunch of people have already been trading using
existing private credit relationships, and they want to participate in a lift
to relieve built up credits so they can keep going.

But there is another kind of lift possible that travels only in a straight
line.  See the figure on page 3 of the accompanying Lifts.odg document
entitled "Linear Lift Example."

In this example, entity D meets entity A through some trading opportunity (like
an online auction site, for example).  Due to the reputation of the site, D
trusts A enough to believe he will actually ship the product or services so now
D wants to pay A.

However, they don't know each other well enough to establish a trading tally
between them.  They just want to get this one transaction done, and that will
be the end of it.

In this case, we don't need a complete circuit of credits.  We just need
"nearly a circuit" or a circuit, with the exclusion of a direct credit link 
between A and D.

In this case, no credit transaction will flow between A and D.  We have the
product or services to account for this.  We will just need to negotiate one
or more lifts which add up exactly to the value meant to be transmitted from
D to A.  If we can find enough cooperating parties willing to do so in a path
that starts with D and ends with A, we can complete the transaction.

In nearly every other way, this is identical to a circular lift.  But the
special case does need to be accommodated.  In fact, it may likely become the
more prevalent form of lift.

-------------------------------------------------------------------------------
Lift Algorithm:

If lifts were to be discovered and executed in real time, the process would be
something like the following:

- Query foils (downstream tallies) that are below target levels
  - A negative number is credit that needs to be paid at least back to zero.
  - Some foil tallies will have higher (positive) targets to accumulate a debit 
    balance so it can be spent later.
- Identify the target foil to lift (we will call this vendor peer V)
- Compare the amount desired to lift for the target foil to available 
  accumulations on active stocks.
- Pick a stock (upstream tally) to target for the lift.
- Query the upstream peer for that stock (customer peer C) to see if he knows 
  how to reach V upstream somewhere.
- If our query goes unanswered, or answered negatively, try to select another
  eligible stock and repeat the query with that upstream peer.
- If/when we get an affirmative answer back from an upstream peer, proceed 
  to initiate a lift.

- The original search query has a UUID that may have been cached upstream.
- Promote the status to "request", insert the actual amount, and repeat the 
  query.
- This request should propagate all the way around the circle and come back to
  us via peer V.

-------------------------------------------------------------------------------
Lift Calculation:

In reality, it wouldn't be very efficient to recalculate every lift pathway
from scratch.  Rather, we should populate a database with information about our
own local users and the foreign users they share tallies with that will help 
us figure out pathways in advance that are likely to succeed.  Successful 
pathways can be used over and over again.

Our local database already contains information about how our local users are
related to each other.  If they share a tally, we know that.  And we know who
holds the stock and who holds the foil.

In addition, we have data about certain foreign peers.  Although we don't host 
these users, we still know about them because they share tallies with users in
our system.

What we would like to collect is data about how these foreign users might be
linked to each other in the outside world.  But those details may be private 
and not something others want to share with the world.  So what we can settle
for is just the knowledge that a pathway exists, even if we don't know all the
parties along that pathway.

For example, see the figure on page 4 entitled "Distributed Lift Example."  We
certainly know about our own users A, and B.  We also know some things about
users C and D.  But we don't really know who C and D are connected to in the
outside world, or if there is a lift pathway from D to C.

If C and D are both hosted by another single database, it would be pretty easy
to just ask: "Hey, are C and D connected?  And is a lift possible going 
through D and eventually getting to C?"  That database could answer the
question just as easily as we can for our own interconnected users.  And it
would not really have to give us much information, other than to say that a
lift along that path is possible.

But what if they are not on the same server?  That is where it gets more
complicated.  Essentially, D's host database would have to note that C is not
its customer, but that it would be useful to be able to find an upstream
connection to C, wherever in the world it might be hosted.  And so it could
pass the query on to those foreign hosts it knows about who are upstream from
D.

If one of those hosts C, it can answer and D's host can then relay the
information back to A's host.  If not, they can pass the inquiry along for as
long as it takes until either there are no more upstream pathways to query
or C's host is eventually discovered.

In practice we hope to deliver some hints as well that will make this process
more efficient so hosts don't have to continue querying off in useless 
directions.

-------------------------------------------------------------------------------
Routes

The route table is used for this purpose.  It is designed to model two 
different cases:

Native:
A native route represents a hypothetical link we would like to discover which
exists outside our own data set which completes a partial circuit we already 
know about.

As an example, see Peers D and C in the figure.  We would like to find an
external upstream pathway which leads from D to C.  Since we have peer records
for both peers, we can represent them by their local ID's in our system.  But
in order to discover if the proposed pathway is possible, we will have to start
by querying the system that hosts D.  That leads us to the next case:

Relay:
On D's system, D is certainly well known as a user there.  A is also known, but
as a foreign peer--not a local user.  But C may not be known at all.  So when
we ask D's system to help us find the route, it will need a way to store the
route information even though it has no local record for C.  In this case, it
will just store the CHIP address and not a local relational link.

Two further complications:  A relay record is just a query from a downstream
system.  We really don't need to know about this path for the purpose of
initiating our own lifts.  Rather, we hope to get lifts coming from downstream
through this channel.  Since we don't know where C is ourselves, the only way
we can hope to find out is by sending more relay requests to our own upstream
peers.  If/when an answer does come back, we will need to remember enough
information about our downstream requester so we can tell him properly whether
the route has succeeded or failed.

So our complete lift information system consists of two basic data sets:

The view tallies_v_paths: which links all our known tallies together in a
queriable chain of internal pathways; and the table/view routes/routes_v which 
contains both native and relay routes.

Note the "sense of flow" in tallies_v_paths is downstream.  That is, the path
array begins with upstream peers and proceeds downstream.

By convention, the route table has more of an upstream feel to its flow.  That
is, it begins with a foreign peer known to our system, and reaches upward to 
find another external peer we are searching for.

Model Requirements:
Native:
  Route Entity: The local ID of a foreign peer (D) who holds a foil tally with 
  	one of our users (A).
  Destination Entity: The local ID of a foreign peer (C) who holds a stock 
  	tally with one of our users (B) and can be found at the end of a chain 
  	which starts with the Route Entity (D).
  Route Key: Some identifier that uniquely identifies this route record which
  	we will pass to our upstream peer.  If the route is found to be good,
  	we expect this key to be returned to us so we know which record to 
  	mark as good.  This might just consist of the Route and Destination
  	CHIP addresses.  It should not consist of our internal ID's for those
  	peers as that might compromise otherwise private information.

Relay:
  Route Entity: The local ID of a foreign peer on our system who holds a foil
  	tally with one of our users.  This is not the same as the Route Entity
  	from the downstream system that asked us to relay this route request.
  	Rather this is a peer at the top of a known chain in our system.
  Destination Peer: This could potentially include a local ID if we happen
  	to know about this peer.  More likely, it is someone we don't have
  	direct knowledge about, so we will have to store it as a CHIP address
  	only.  This is the same peer as the Destination Entity represented in
  	the original native route request that came from somewhere downstream.
  Route Key: Similar to above, but unique to this record on this system.
  Return Socket: A connection point (host and port) where we will connect to 
  	return status updates about this route request.
  Return Key: The Record Key data originally sent from the downstream 
  	requesting system.  We will return this data when reporting on status
  	updates about this route.
-------------------------------------------------------------------------------
Test Case: (see figures on page 5)

Native:			(On david_meese's system: lux1)
  Route Entity:		p1004: kathleen_ramsey@lux0		route_ent
  Destination Entity:	p1003: lakisha_villiard@lux2	dest_ent,dest_chid/host
  Route Key:		kathleen_ramsey@lux0 (+lakisha)		(inherent)
  Return Gateway:	p1001: david_meese@lux1			topu_ent
  Return Key:		p1001: david_meese@lux1			botu_ent
  Return Socket:	p1001: david_meese@lux1	(inside=native)	requ_ent

Relay 1:		(On kathleen_ramsey's system: lux0)
  Route Entity:		p1004: tricia_mayo@lux3			route_ent
  Destination Peer:	       lakisha_villiard@lux2		dest_chid/host
  Route Key:		tricia_mayo@lux3 (+lakisha)		(inherent)
  Return Gateway:	p1002: gena_avila@lux0			topu_ent
  Return Key:		p1000: kathleen_ramsey@lux0 (+lakisha)	botu_ent
  Return Socket:	p1005: david_meese@lux1:65430		requ_ent

Relay 2:		(On tricia_mayo's system: lux3)
  Route Entity:		p1003: stacey_welch@lux2		route_ent
  Destination Peer:	       lakisha_villiard@lux2		dest_chid/host
  Route Key:		stacey_welch@lux2 (+lakisha)
  Return Gateway:	p1000: tricia_mayo@lux3			topu_ent
  Return Key:		p1000: tricia_mayo@lux3 (+lakisha)	botu_ent
  Return Socket:	p1005: gena_avila@lux0:65430		requ_ent

-------------------------------------------------------------------------------
